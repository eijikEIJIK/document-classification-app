{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "train_notebook.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ye2eDXT9UPZf"
      },
      "outputs": [],
      "source": [
        "%cd /content/drive/MyDrive/Colab/Trial"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout, Activation, Flatten\n",
        "from tensorflow.keras.preprocessing.image import array_to_img, img_to_array, load_img, ImageDataGenerator\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import sys\n",
        "import os\n",
        "import shutil\n",
        "import math\n",
        "from IPython.display import Image as IPImage\n",
        "from IPython.display import display_jpeg\n",
        "\n",
        "print(tf.__version__)\n",
        "\n",
        "train_dir = 'target_datasets/train'\n",
        "val_dir = 'target_datasets/val'\n",
        "\n",
        "backup_dir = './model'\n",
        "\n",
        "labels = [d for d in os.listdir(train_dir) if os.path.isdir(os.path.join(train_dir, d))]\n",
        "labels.sort()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8XNX_9MuUhq3",
        "outputId": "9f3d56c7-53ec-4541-fad9-6c4c87180505"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.7.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#シードの設定\n",
        "seed=1234\n",
        "initializer=tf.keras.initializers.GlorotUniform(seed=seed)\n",
        "\n",
        "NUM_CLASSES = len(labels)\n",
        "print(\"ラベル数：\"+str(NUM_CLASSES))\n",
        "# 学習率\n",
        "LEARNING_RATE = 0.0005\n",
        "# エポック（世代数）\n",
        "EPOCHS = 24\n",
        "# バッチサイズ\n",
        "BATCH_SIZE = 16\n",
        "\n",
        "IMAGE_SIZE = 256\n",
        "\n",
        "train_data_gen = ImageDataGenerator(rescale=1./255,\n",
        "                                    width_shift_range=0.1,\n",
        "                                    height_shift_range=0.1,\n",
        "                                    zoom_range=0.1,\n",
        "                                    horizontal_flip=True,)\n",
        "val_data_gen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "train_data = train_data_gen.flow_from_directory(\n",
        "    train_dir, target_size=(IMAGE_SIZE, IMAGE_SIZE),\n",
        "    color_mode='rgb', batch_size=BATCH_SIZE,\n",
        "    class_mode='categorical', shuffle=True)\n",
        "\n",
        "validation_data = val_data_gen.flow_from_directory(\n",
        "    val_dir, target_size=(IMAGE_SIZE, IMAGE_SIZE),\n",
        "    color_mode='rgb', batch_size=BATCH_SIZE,\n",
        "    class_mode='categorical', shuffle=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k54hhF1FUlGV",
        "outputId": "7eddfca9-8742-4cf0-fb8d-121b1249bae3"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ラベル数：7\n",
            "Found 296 images belonging to 7 classes.\n",
            "Found 76 images belonging to 7 classes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = Sequential()\n",
        "\n",
        "model.add(Conv2D(16, (3, 3), padding='same',\n",
        "    input_shape=(IMAGE_SIZE, IMAGE_SIZE, 3)))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model.add(Activation('relu'))\n",
        "\n",
        "model.add(Conv2D(16, (3, 3)))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model.add(Activation('relu'))\n",
        "\n",
        "model.add(Conv2D(16, (3, 3)))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model.add(Activation('relu'))\n",
        "\n",
        "model.add(Conv2D(16, (3, 3)))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model.add(Activation('relu'))\n",
        "\n",
        "model.add(Dropout(rate=0.25))\n",
        "\n",
        "model.add(Flatten())\n",
        "model.add(Dense(128))\n",
        "model.add(Activation('relu'))\n",
        "\n",
        "model.add(Dropout(rate=0.2))\n",
        "\n",
        "model.add(Dense(NUM_CLASSES))\n",
        "model.add(Activation('softmax'))\n",
        "\n",
        "opt = tf.keras.optimizers.Adam(lr=LEARNING_RATE)\n",
        "\n",
        "model.compile(opt, loss='categorical_crossentropy',metrics=['accuracy'])\n",
        "model.summary()\n",
        "\n",
        "# Execute train\n",
        "history = model.fit(train_data, epochs=EPOCHS, validation_data=validation_data, verbose=1)\n",
        "\n",
        "score = model.evaluate(validation_data)\n",
        "print('Test loss:', score[0])\n",
        "print('Test accuracy:', score[1])\n",
        "\n",
        "# Save model\n",
        "save_model_path = os.path.join(backup_dir, 'model_1221.h5')\n",
        "model.save(save_model_path)"
      ],
      "metadata": {
        "id": "1ynBEszmUvX_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%matplotlib inline\n",
        "from PIL import Image\n",
        "import glob\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.image as mpimg\n",
        "\n",
        "def get_category(x):\n",
        "  if len(x)!=7:\n",
        "    return 0\n",
        "  max_index = np.argmax(x)\n",
        "  cat_list=[\"anime\",\"building\",\"document\",\"food\",\"mono\",\"person\",\"snowscene\"]\n",
        "\n",
        "  return cat_list[max_index]\n",
        "\n",
        "img_li=[]\n",
        "files = glob.glob(\"predict_data/*\")\n",
        "for i,img_name in enumerate(files):\n",
        "  im = tf.io.read_file(img_name)\n",
        "  im = tf.image.decode_jpeg(im, channels=3)\n",
        "  #画像のリサイズ\n",
        "  im=tf.image.resize(im, [IMAGE_SIZE, IMAGE_SIZE])/255\n",
        "  img_li.append(im)\n",
        "img_li=np.array(img_li)\n",
        "\n",
        "model_path = os.path.join(backup_dir, 'model_1221.h5')\n",
        "new_model=tf.keras.models.load_model(model_path)\n",
        "results=new_model.predict(img_li)\n",
        "\n",
        "for i in range(len(results)):\n",
        "  img_name=files[i]\n",
        "  print(img_name)\n",
        "  img = mpimg.imread(img_name)\n",
        "  imgplot = plt.imshow(img)\n",
        "  plt.show()\n",
        "  print(get_category(results[i]))"
      ],
      "metadata": {
        "id": "R28Q5y0oU7UR"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}